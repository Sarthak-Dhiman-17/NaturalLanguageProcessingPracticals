{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ishan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ishan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer :\n",
      "['My', 'name', 'is', 'Sarthak', ',', 'UID', '22BAI70558', '&', 'email', '22BAI70558', '@', 'cuchd.in', ',', 'it', \"'s\", 'https', ':', '//monkeytype.com', ',', '821-977-7727', ',', '9.78']\n",
      "\n",
      "Sentence Tokenization:\n",
      "[\"My name is Sarthak, UID 22BAI70558 & email 22BAI70558@cuchd.in, it's https://monkeytype.com, 821-977-7727, 9.78\"]\n",
      "\n",
      "RegExp Tokenization (Alphabets Only):\n",
      "['My', 'name', 'is', 'Sarthak', 'UID', 'email', 'cuchd', 'in', 'it', 's', 'https', 'monkeytype', 'com']\n",
      "\n",
      "RegExp Tokenization (Alphabets and Digits):\n",
      "['My', 'name', 'is', 'Sarthak', 'UID', '22BAI70558', 'email', '22BAI70558', 'cuchd', 'in', 'it', 's', 'https', 'monkeytype', 'com', '821', '977', '7727', '9', '78']\n",
      "\n",
      "Tweet Tokenization:\n",
      "['This', 'is', 'a', '#50th', 'tweet', 'with', '@silvioorton', 'and', 'a', 'URL', ':', 'https://google.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n",
    "sample_text = \"My name is Sarthak, UID 22BAI70558 & email 22BAI70558@cuchd.in, it's https://monkeytype.com, 821-977-7727, 9.78\"\n",
    "\n",
    "#word tokenizer\n",
    "words = word_tokenize(sample_text) \n",
    "print(\"Word Tokenizer :\")\n",
    "print(words)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(sample_text) \n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(sentences)\n",
    "\n",
    "# Regular Expression Tokenization\n",
    "#3 Tokenize based on words containing alphabets\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\b[a-zA-Z]+\\b')\n",
    "print(\"\\nRegExp Tokenization (Alphabets Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "# Tokenize based on words containing alphabets and digits\n",
    "regexp_tokens_with_digits = regexp_tokenize(sample_text, pattern=r'\\b\\w+\\b|\\d+') \n",
    "print(\"\\nRegExp Tokenization (Alphabets and Digits):\")\n",
    "print(regexp_tokens_with_digits)\n",
    "\n",
    "# Tweet Tokenization (Handles mentions, hashtags, and URLs) tweet_tokenizer = TweetTokenizer()\n",
    "th = TweetTokenizer(strip_handles=False, reduce_len=False)\n",
    "tweet_tokens = th.tokenize(\"This is a #50th tweet with @silvioorton and a URL: https://google.com\")\n",
    "print(\"\\nTweet Tokenization:\") \n",
    "print(tweet_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RegExp Tokenization (Whole words):\n",
      "['My', 'name', 'is', 'Sarthak', 'UID', '22BAI70558', 'email', '22BAI70558', 'cuchd', 'in', 'it', 's', 'https', 'monkeytype', 'com', '821', '977', '7727', '9', '78']\n",
      "\n",
      "RegExp Tokenization (Email Only):\n",
      "['22BAI70558@cuchd.in']\n",
      "\n",
      "RegExp Tokenization (Website Only):\n",
      "['https://monkeytype.com,']\n",
      "\n",
      "RegExp Tokenization (Punctuation Only):\n",
      "[',', '&', '@', '.', ',', \"'\", ':', '/', '/', '.', ',', '-', '-', ',', '.']\n",
      "\n",
      "RegExp Tokenization (Words and Punctuations are separate tokens):\n",
      "['My', 'name', 'is', 'Sarthak', ',', 'UID', '22BAI70558', '&', 'email', '22BAI70558', '@', 'cuchd', '.', 'in', ',', 'it', \"'\", 's', 'https', ':', '/', '/', 'monkeytype', '.', 'com', ',', '821', '-', '977', '-', '7727', ',', '9', '.', '78']\n",
      "\n",
      "RegExp Tokenization (White space Only):\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "\n",
      "RegExp Tokenization (Words including hyphens):\n",
      "['My', 'name', 'is', 'Sarthak', 'UID', 'BAI', 'email', 'BAI', 'cuchd', 'in', \"it's\", 'https', 'monkeytype', 'com', '-', '-']\n",
      "\n",
      "RegExp Tokenization (Matches Number):\n",
      "['821', '977', '7727', '9', '78']\n",
      "\n",
      "RegExp Tokenization (Float numbers Only):\n",
      "['', '', '', '.78']\n",
      "\n",
      "RegExp Tokenization (Match Hashtags):\n",
      "[]\n",
      "\n",
      "RegExp Tokenization (Match mentions):\n",
      "['@cuchd']\n",
      "\n",
      "RegExp Tokenization (Match any tokens):\n",
      "['-', '-']\n",
      "\n",
      "RegExp Tokenization (Custom tokens):\n",
      "['My', 'name', 'is', 'Sarthak', 'UID', '22BAI70558', '&', 'email', '22BAI70558', 'cuchd', 'in', 'it', 's', 'https', 'monkeytype', 'com', '821', '977', '7727', '9', '78']\n"
     ]
    }
   ],
   "source": [
    "#1 whole word\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\b\\w+\\b') \n",
    "print(\"\\nRegExp Tokenization (Whole words):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "\n",
    "#6 Email only\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "print(\"\\nRegExp Tokenization (Email Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#7 Website only\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'https?://[^\\s]+') \n",
    "print(\"\\nRegExp Tokenization (Website Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#8 Punctuation only\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'[^\\w\\s]') \n",
    "print(\"\\nRegExp Tokenization (Punctuation Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#9 words and punctuations are separate tokens\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\b\\w+\\b|[^\\w\\s]') \n",
    "print(\"\\nRegExp Tokenization (Words and Punctuations are separate tokens):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#10 White space only\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\s+') \n",
    "print(\"\\nRegExp Tokenization (White space Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#2 Words with hyphens\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r\"[a-zA-Z'-]+\") \n",
    "print(\"\\nRegExp Tokenization (Words including hyphens):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#4 Matches numbers\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\b\\d+\\b') \n",
    "print(\"\\nRegExp Tokenization (Matches Number):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#5 Float numbers\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\b\\d+(\\.\\d+)?\\b') \n",
    "print(\"\\nRegExp Tokenization (Float numbers Only):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#11 Match hashtags \n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'#\\w+') \n",
    "print(\"\\nRegExp Tokenization (Match Hashtags):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#11 Match mentions \n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'@\\w+') \n",
    "print(\"\\nRegExp Tokenization (Match mentions):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#12 match any token ( such as - )\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'\\-+') \n",
    "print(\"\\nRegExp Tokenization (Match any tokens):\")\n",
    "print(regexp_tokens)\n",
    "\n",
    "#13 Custom Tokens with specific symbols\n",
    "regexp_tokens = regexp_tokenize(sample_text, pattern=r'[a-zA-Z0-9]+|[&]') \n",
    "print(\"\\nRegExp Tokenization (Custom token for symbol & with words):\")\n",
    "print(regexp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Sarthak', ',', 'today', 'india', 'won', 'champions', 'trophy', '3', 'times']\n",
      "['My name is Sarthak , today india won champions trophy 3 times']\n",
      "['My', 'name', 'is', 'Sarthak', 'today', 'india', 'won', 'champions', 'trophy', 'times']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, TweetTokenizer\n",
    "\n",
    "sample = \"My name is Sarthak , today india won champions trophy 3 times\"\n",
    "word = word_tokenize(sample)\n",
    "print(word)\n",
    "\n",
    "word = sent_tokenize(sample)\n",
    "print(word)\n",
    "\n",
    "re = regexp_tokenize(sample,pattern=r'\\b[a-zA-Z]+\\b')\n",
    "print(re)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
